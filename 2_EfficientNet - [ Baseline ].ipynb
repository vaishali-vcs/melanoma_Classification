{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training EfficientNet model on TPU\n",
    "\n",
    "This notebook trains Efficient model on TPU hardware and also creates a submission file that will be uploaded on kaggle comeptition.\n",
    "\n",
    "Contents-\n",
    "1. Introduction to TPU<br>\n",
    "    1.1. TPU on Kaggle<br>\n",
    "    1.2 Disadvantages of TPU<br>\n",
    "2. Train the model\n",
    "3. Test File Submission\n",
    "4. References\n",
    "\n",
    "\n",
    "## 1. Introduction to TPU\n",
    "\n",
    "A tensor processing unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google specifically for neural network machine learning, particularly using Google's own TensorFlow software. Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale. **Below is a picture of TPU hardware version3.8** \n",
    "\n",
    "<img src=\"./images/tpu_cores_and_chips.png\" width=620 height=620 title=\"TPU v3-8 hardware, 4 chips, 8 cores\" />\n",
    "At approximately 20 inches (50 cm), a TPU v3-8 board is a fairly sizeable piece of hardware. It sports 4 dual-core TPU chips for a total of 8 TPU cores.\n",
    "\n",
    "Each TPU core has a traditional vector processing part (VPU) as well as dedicated matrix multiplication hardware capable of processing 128x128 matrices. This is the part that specifically accelerates machine learning workloads.\n",
    "\n",
    "TPUs are equipped with 128GB of high-speed memory allowing larger batches, larger models and also larger training inputs. In the sample above, you can try using 512x512 px input images, also provided in the dataset, and see the TPU v3-8 handle them easily.\n",
    "\n",
    "Compared to a graphics processing unit, it is designed for a high volume of low precision computation (e.g. as little as 8-bit precision) with more input/output operations per joule, and lacks hardware for rasterisation/texture mapping. The TPU ASICs are mounted in a heatsink assembly, which can fit in a hard drive slot within a data center rack.\n",
    "\n",
    "### 1.1. TPU on Kaggle\n",
    "**Google doesnot sell the TPUs commercially.** But, they announced that the second gen of the TPUs will be accessible through the Google cloud computing platform. Basically you have to rent it, similar to the Amazon's AWS (Amazon machine learning platform).\n",
    "TPUs are now available on Kaggle, for free. They are supported in Tensorflow 2.1 both through the Keras high-level API and, at a lower level, in models using a custom training loop.\n",
    "\n",
    "In our case, we will be using the free quota of TPU available on Kaggle.\n",
    "\n",
    "### 1.2 Disadvantages of TPU\n",
    "There are a few drawbacks to be aware of:\n",
    "\n",
    " - The topology is unlike other hardware platforms and is not trivial to work with for those not familiar with DevOps and the idiosyncrasies of the TPU itself\n",
    " - The TPU only supports TensorFlow currently, although other frameworks may be supported in the future   \n",
    " - Certain TensorFlow operations (e.g. customer operations written in C++) are not supported\n",
    " - TPUs are optimal for large models with very large batch sizes and workloads that are dominated by matrix-multiplication. Models dominated by algebra will not perform well.\n",
    " \n",
    "\n",
    "# 2. Train the model\n",
    "\n",
    "In this section we will,\n",
    "- load the data \n",
    "- intialize the TPU\n",
    "- train EfficientNet model.\n",
    "\n",
    "The below code installs the necessary library\n",
    "## 2.1 Steps to setup TPU in kaggle/Google notebook\n",
    "Once you have flipped the \"Accelerator\" switch in your notebook to \"TPU v3-8\", this is how to enable TPU training in Tensorflow Keras:\n",
    "1. TPUs are network-connected accelerators and you must first locate them on the network. This is what _TPUClusterResolver()_ does.\n",
    "\n",
    "2. Two additional lines of boilerplate are required to define a _TPUStrategy_. This object contains the necessary distributed training code that will work on TPUs with their 8 compute cores.\n",
    "\n",
    "3. Finally, use the _TPUStrategy_ by instantiating your model in the scope of the strategy. This creates the model on the TPU. Model size is constrained by the TPU RAM only, not by the amount of memory available on the VM running your Python code. Model creation and model training use the usual Keras APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "## 2.2 Load data\n",
    "The below code does the following:\n",
    "1. imports necessary libraries\n",
    "2. locate the TPU on the network\n",
    "3. connect to the searched TPU\n",
    "4. intialize the tpu environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import efficientnet.tfkeras as efn\n",
    "import dill\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "print('Running on TPU ', tpu.master())\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that TPU address is grpc://10.0.0.2:8470. Now, the next step is to load the dataset and set up the model's hyperparameters. The below code does the following jobs\n",
    "\n",
    "1. Set the prefetch strategy as AUTOTUNE. Prefetching overlaps the preprocessing and model execution of a training step. It can be used to decouple the time when data is produced from the time when data is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. But,to tf.data.experimental.AUTOTUNE which will help the runtime to tune the value dynamically at runtime.\n",
    "2. epochs = 15, learning rate = 1e-3, input image size as 384 X 384.\n",
    "3. fetch the training & test records in 'tfrec' format. Also fetch the submission csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('melanoma-384x384')\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "AUG_BATCH = BATCH_SIZE\n",
    "IMAGE_SIZE = [384, 384]\n",
    "# Seed\n",
    "SEED = 333\n",
    "# Learning rate\n",
    "LR = 1e-3\n",
    "# cutmix prob\n",
    "cutmix_rate = 0.30\n",
    "\n",
    "# training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n",
    "# test filenames directory\n",
    "TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\n",
    "# submission file\n",
    "submission = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below helper functions are useful in seeding the environment, load the datasets in batches and convert the byte images into jpeg and normalize the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def load_dataset_full(filenames):        \n",
    "    # automatically interleaves reads from multiple files\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    # returns a dataset of (image_name, target)\n",
    "    dataset = dataset.map(read_tfrecord_full, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "def get_data_full(filenames):\n",
    "    dataset = load_dataset_full(filenames)\n",
    "    dataset = dataset.map(setup_input3, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "# function to decode our images (normalize and reshape)\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    # convert image to floats in [0, 1] range\n",
    "    image = tf.cast(image, tf.float32) / 255.0 \n",
    "    # explicit size needed for TPU\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below functions are helpful in combining the metadata and the images into single TF record that is later input to the model. This also converts the variable 'anatom_site_general_challenge' to categorical values.\n",
    "This is done for both Train and Test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training and validation dataset\n",
    "def setup_input1(image, label, data):\n",
    "    \n",
    "    # get anatom site general challenge vectors\n",
    "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
    "    \n",
    "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
    "    \n",
    "    tabular = tf.stack(tab_data + anatom)\n",
    "    \n",
    "    return {'inp1': image, 'inp2':  tabular}, label\n",
    "\n",
    "# function for the test set\n",
    "def setup_input2(image, image_name, data):\n",
    "    \n",
    "    # get anatom site general challenge vectors\n",
    "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
    "    \n",
    "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
    "    \n",
    "    tabular = tf.stack(tab_data + anatom)\n",
    "    \n",
    "    return {'inp1': image, 'inp2':  tabular}, image_name\n",
    "\n",
    "# function for the validation (image name)\n",
    "def setup_input3(image, image_name, target, data):\n",
    "    \n",
    "    # get anatom site general challenge vectors\n",
    "    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n",
    "    \n",
    "    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n",
    "    \n",
    "    tabular = tf.stack(tab_data + anatom)\n",
    "    \n",
    "    return {'inp1': image, 'inp2':  tabular}, image_name, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below 3 functions input the dataset to the model by batches diregarding data order. Order does not matter since we will be shuffling the data anyway. They also prefetch the next batch of samples while training the current sample to fasten the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(filenames, labeled = True, ordered = False):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(transform, num_parallel_calls = AUTO)\n",
    "    # the training dataset must repeat for several epochs\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(filenames, labeled = True, ordered = True):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # using gpu, not enought memory to use cache\n",
    "    # dataset = dataset.cache()\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(filenames, labeled = False, ordered = True):\n",
    "    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n",
    "    dataset = dataset.map(setup_input2, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "  \n",
    "def load_dataset(filenames, labeled = True, ordered = False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        # disable order, increase speed\n",
    "        ignore_order.experimental_deterministic = False \n",
    "        \n",
    "    # automatically interleaves reads from multiple files\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    # use data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below functions returns a pair of a train/test sample in the form (image, label, images' metadata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        \n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['target'], tf.float32)\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    # returns a dataset of (image, label, data)\n",
    "    return image, label, data\n",
    "\n",
    "# this function parse our image and also get our image_name (id) to perform predictions\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        # tf.string means bytestring\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        # shape [] means single element\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image_name = example['image_name']\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    # returns a dataset of (image, key, data)\n",
    "    return image, image_name, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below functions returns a pair of a train/test sample in the form (image, image name,label, images' metadata). We also have another helper function to do data augmentation. Following are the augmentations done.\n",
    "1. randomly flip image left/right & up/down\n",
    "2. randomly set hue\n",
    "3. randomly set saturation\n",
    "4. randomly set contrast\n",
    "5. randomly set brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(data, label):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement \n",
    "    # in the next function (below), this happens essentially for free on TPU. \n",
    "    # Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    data['inp1'] = tf.image.random_flip_left_right(data['inp1'])\n",
    "    data['inp1'] = tf.image.random_flip_up_down(data['inp1'])\n",
    "    data['inp1'] = tf.image.random_hue(data['inp1'], 0.01)\n",
    "    data['inp1'] = tf.image.random_saturation(data['inp1'], 0.7, 1.3)\n",
    "    data['inp1'] = tf.image.random_contrast(data['inp1'], 0.8, 1.2)\n",
    "    data['inp1'] = tf.image.random_brightness(data['inp1'], 0.1)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "\n",
    "# this function parse our images and also get the target variable\n",
    "def read_tfrecord_full(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), \n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string), \n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64), \n",
    "        # meta features\n",
    "        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    image_name = example['image_name']\n",
    "    target = tf.cast(example['target'], tf.float32)\n",
    "    # meta features\n",
    "    data = {}\n",
    "    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n",
    "    data['sex'] = tf.cast(example['sex'], tf.int32)\n",
    "    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n",
    "    return image, image_name, target, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function returns the transformation matrix that is to be applied on the batches of train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below helper functions applies the transformation matrix on the entire batch of train samples. The 3rd data augmentation strategy is called \"data CutMix augmentation strategy\" where patches are cut and pasted among training images. \n",
    "\n",
    "In the CutMix algorithm, for each image of batch a random region (in our case, a random bounding box region) is replaced with a patch from another training image.\n",
    "The parameter lambda (the variable p in the code) that determines the size of the bounding box is stochastically sampled from a a distribution.\n",
    "The label of each resultant augmented image is estimated as a weighted sum of the original label and the label of the image from which the modified patch is borrowed.\n",
    "\n",
    "This technique has been proven to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 26153 training images, 6538 validation images, 10982 unlabeled test images\n"
     ]
    }
   ],
   "source": [
    "def transform(image, label):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
    "        rot = 15. * tf.random.normal([1],dtype='float32')\n",
    "    else:\n",
    "        rot = 180. * tf.random.normal([1],dtype='float32')\n",
    "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
    "        shr = 5. * tf.random.normal([1],dtype='float32') \n",
    "    else:\n",
    "        shr = 2. * tf.random.normal([1],dtype='float32')\n",
    "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
    "        h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10. \n",
    "    else:\n",
    "        h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/8.\n",
    "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
    "        w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10. \n",
    "    else:\n",
    "        w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/8.\n",
    "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
    "        h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "    else:\n",
    "        h_shift = 8. * tf.random.normal([1],dtype='float32')\n",
    "    if 0.5 > tf.random.uniform([1], minval = 0, maxval = 1):\n",
    "        w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "    else:\n",
    "        w_shift = 8. * tf.random.normal([1],dtype='float32')\n",
    "  \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image['inp1'],tf.transpose(idx3))\n",
    "        \n",
    "    return {'inp1': tf.reshape(d,[DIM,DIM,3]), 'inp2': image['inp2']}, label\n",
    "\n",
    "# function to apply cutmix augmentation\n",
    "def cutmix(image, label):\n",
    "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
    "    # output - a batch of images with cutmix applied\n",
    "    \n",
    "    DIM = IMAGE_SIZE[0]    \n",
    "    imgs = []; labs = []\n",
    "    \n",
    "    for j in range(BATCH_SIZE):\n",
    "        \n",
    "        #random_uniform( shape, minval=0, maxval=None)        \n",
    "        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
    "        P = tf.cast(tf.random.uniform([], 0, 1) <= cutmix_rate, tf.int32)\n",
    "        \n",
    "        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
    "        k = tf.cast(tf.random.uniform([], 0, BATCH_SIZE), tf.int32)\n",
    "        \n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n",
    "        y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n",
    "        \n",
    "        # Beta(1, 1)\n",
    "        b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\n",
    "        \n",
    "\n",
    "        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        \n",
    "        # MAKE CUTMIX IMAGE\n",
    "        one = image['inp1'][j,ya:yb,0:xa,:]\n",
    "        two = image['inp1'][k,ya:yb,xa:xb,:]\n",
    "        three = image['inp1'][j,ya:yb,xb:DIM,:]        \n",
    "        #ya:yb\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "\n",
    "        img = tf.concat([image['inp1'][j,0:ya,:,:],middle,image['inp1'][j,yb:DIM,:,:]],axis=0)\n",
    "        imgs.append(img)\n",
    "        \n",
    "        # MAKE CUTMIX LABEL\n",
    "        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
    "        lab1 = label[j,]\n",
    "        lab2 = label[k,]\n",
    "        labs.append((1-a)*lab1 + a*lab2)\n",
    "\n",
    "    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n",
    "    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE, 1))\n",
    "    return {'inp1': image2, 'inp2': image['inp2']}, label2\n",
    "\n",
    "\n",
    "# function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "\n",
    "NUM_TRAINING_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.8)\n",
    "# use validation data for training\n",
    "NUM_VALIDATION_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.2)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "\n",
    "print('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that the correct number of train, val and test images are loaded. Now, we will define a function to define a new type of Loss Function called \"Binary Focal Loss\".\n",
    "\n",
    "Focal Loss is binary cross-entropy by introducing a hyperparameter γ (gamma), called the focusing parameter. This allows hard-to-classify examples to be penalized more heavily relative to easy-to-classify examples. The focal loss is defined as\n",
    "\\begin{align}L(y, \\hat{p})\n",
    "= -\\alpha y \\left(1 - \\hat{p}\\right)^\\gamma \\log(\\hat{p})\n",
    "- (1 - y) \\hat{p}^\\gamma \\log(1 - \\hat{p})\\end{align} Where,\n",
    "- $ y \\in \\{0, 1\\} $ is a binary class label,\n",
    "- $ \\hat{p} \\in [0, 1 $  is an estimate of the probability of the positive class,\n",
    "- $ \\gamma $  is the focusing parameter that specifies how much higher-confidence correct predictions contribute to the overall loss (the higher the γ, the higher the rate at which easy-to-classify examples are down-weighted). We have chosen this as 2.\n",
    "- $ \\alpha $  is a hyperparameter that governs the trade-off between precision and recall by weighting errors for the positive class up or down ($ \\alpha $  is the default, which is the same as no weighting). We have chosen this as .8 \n",
    "\n",
    "The usual weighted binary cross-entropy loss is recovered by setting $ \\gamma = 0 $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code modifies the EfficientNetB3 to suit our Binary Classification by adding 4 Dense Layers and adding \"sigmoid\" activation for the output layer. Following are the model hyperparameters used.\n",
    "1. Adam Optimizer\n",
    "2. Metrics to be tracked is BinaryAccuracy and AUC\n",
    "3. focal_loss with gamma = 2.0 & alpha = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    \n",
    "    with strategy.scope():\n",
    "        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n",
    "        efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n",
    "        x = efnetb3(inp1)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x1 = tf.keras.layers.Dense(50)(inp2)\n",
    "        x1 = tf.keras.layers.BatchNormalization()(x1)\n",
    "        x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "        concat = tf.keras.layers.concatenate([x, x1])\n",
    "        concat = tf.keras.layers.Dense(512, activation = 'relu')(concat)\n",
    "        concat = tf.keras.layers.BatchNormalization()(concat)\n",
    "        concat = tf.keras.layers.Dropout(0.2)(concat)\n",
    "        concat = tf.keras.layers.Dense(182, activation = 'relu')(concat)\n",
    "        concat = tf.keras.layers.BatchNormalization()(concat)\n",
    "        concat = tf.keras.layers.Dropout(0.3)(concat)\n",
    "        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "        # opt = tfa.optimizers.SWA(opt)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n",
    "            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code Trains the model and also predicts on Test images.\n",
    "\n",
    "1. Split the data into 5 folds\n",
    "2. For each of the fold, train a EfficientNetB3\n",
    "3. Predict 5 sets of predictions for Test using these 5 models and take their average as the final prediction. This technique is called Out of Fold (OOF) technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "44113920/44107200 [==============================] - 2s 0us/step\n",
      "Epoch 1/15\n",
      "204/204 - 74s - loss: 0.5151 - auc: 0.5603 - binary_accuracy: 0.8228 - val_loss: 0.2242 - val_auc: 0.7200 - val_binary_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "204/204 - 56s - loss: 0.2826 - auc: 0.6305 - binary_accuracy: 0.9524 - val_loss: 1.1960 - val_auc: 0.6625 - val_binary_accuracy: 0.9777 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "204/204 - 58s - loss: 0.2484 - auc: 0.6827 - binary_accuracy: 0.9688 - val_loss: 0.2825 - val_auc: 0.7213 - val_binary_accuracy: 0.9352 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "204/204 - 60s - loss: 0.2184 - auc: 0.7386 - binary_accuracy: 0.9743 - val_loss: 0.2024 - val_auc: 0.8273 - val_binary_accuracy: 0.9818 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "204/204 - 57s - loss: 0.2295 - auc: 0.7313 - binary_accuracy: 0.9733 - val_loss: 0.2034 - val_auc: 0.7657 - val_binary_accuracy: 0.9824 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "204/204 - 58s - loss: 0.2283 - auc: 0.7440 - binary_accuracy: 0.9754 - val_loss: 0.4293 - val_auc: 0.6355 - val_binary_accuracy: 0.9801 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "204/204 - 58s - loss: 0.2131 - auc: 0.7579 - binary_accuracy: 0.9767 - val_loss: 0.2108 - val_auc: 0.8149 - val_binary_accuracy: 0.9774 - lr: 4.0000e-04\n",
      "Epoch 8/15\n",
      "204/204 - 60s - loss: 0.2179 - auc: 0.7647 - binary_accuracy: 0.9739 - val_loss: 0.1813 - val_auc: 0.8295 - val_binary_accuracy: 0.9791 - lr: 4.0000e-04\n",
      "Epoch 9/15\n",
      "204/204 - 58s - loss: 0.2120 - auc: 0.7576 - binary_accuracy: 0.9779 - val_loss: 2.2715 - val_auc: 0.6872 - val_binary_accuracy: 0.7451 - lr: 4.0000e-04\n",
      "Epoch 10/15\n",
      "204/204 - 60s - loss: 0.2134 - auc: 0.7569 - binary_accuracy: 0.9764 - val_loss: 0.1876 - val_auc: 0.8317 - val_binary_accuracy: 0.9809 - lr: 4.0000e-04\n",
      "Epoch 11/15\n",
      "204/204 - 58s - loss: 0.2001 - auc: 0.7934 - binary_accuracy: 0.9781 - val_loss: 0.1846 - val_auc: 0.8252 - val_binary_accuracy: 0.9815 - lr: 4.0000e-04\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "204/204 - 59s - loss: 0.2046 - auc: 0.7761 - binary_accuracy: 0.9776 - val_loss: 0.1965 - val_auc: 0.8225 - val_binary_accuracy: 0.9794 - lr: 4.0000e-04\n",
      "Epoch 13/15\n",
      "204/204 - 60s - loss: 0.1990 - auc: 0.7886 - binary_accuracy: 0.9792 - val_loss: 0.1676 - val_auc: 0.8653 - val_binary_accuracy: 0.9800 - lr: 1.6000e-04\n",
      "Epoch 14/15\n",
      "204/204 - 59s - loss: 0.1927 - auc: 0.8072 - binary_accuracy: 0.9793 - val_loss: 0.1735 - val_auc: 0.8589 - val_binary_accuracy: 0.9818 - lr: 1.6000e-04\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "204/204 - 59s - loss: 0.1928 - auc: 0.8072 - binary_accuracy: 0.9780 - val_loss: 0.1860 - val_auc: 0.8346 - val_binary_accuracy: 0.9815 - lr: 1.6000e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\n",
      "Epoch 1/15\n",
      "204/204 - 72s - loss: 0.5159 - auc: 0.5997 - binary_accuracy: 0.8256 - val_loss: 0.2237 - val_auc: 0.7506 - val_binary_accuracy: 0.9823 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "204/204 - 55s - loss: 0.2814 - auc: 0.6605 - binary_accuracy: 0.9525 - val_loss: 0.2314 - val_auc: 0.7442 - val_binary_accuracy: 0.9823 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "204/204 - 58s - loss: 0.2427 - auc: 0.7117 - binary_accuracy: 0.9684 - val_loss: 0.8902 - val_auc: 0.6587 - val_binary_accuracy: 0.9812 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "204/204 - 61s - loss: 0.2120 - auc: 0.7770 - binary_accuracy: 0.9715 - val_loss: 0.3105 - val_auc: 0.8121 - val_binary_accuracy: 0.9823 - lr: 4.0000e-04\n",
      "Epoch 5/15\n",
      "204/204 - 58s - loss: 0.1977 - auc: 0.8026 - binary_accuracy: 0.9749 - val_loss: 0.5544 - val_auc: 0.7623 - val_binary_accuracy: 0.9821 - lr: 4.0000e-04\n",
      "Epoch 6/15\n",
      "204/204 - 61s - loss: 0.1968 - auc: 0.8127 - binary_accuracy: 0.9735 - val_loss: 0.1760 - val_auc: 0.8490 - val_binary_accuracy: 0.9813 - lr: 4.0000e-04\n",
      "Epoch 7/15\n",
      "204/204 - 60s - loss: 0.1992 - auc: 0.8040 - binary_accuracy: 0.9750 - val_loss: 0.1825 - val_auc: 0.8154 - val_binary_accuracy: 0.9823 - lr: 4.0000e-04\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "204/204 - 59s - loss: 0.2028 - auc: 0.7913 - binary_accuracy: 0.9764 - val_loss: 0.2069 - val_auc: 0.8318 - val_binary_accuracy: 0.9786 - lr: 4.0000e-04\n",
      "Epoch 9/15\n",
      "204/204 - 59s - loss: 0.1875 - auc: 0.8231 - binary_accuracy: 0.9768 - val_loss: 0.1903 - val_auc: 0.8472 - val_binary_accuracy: 0.9810 - lr: 1.6000e-04\n",
      "Epoch 10/15\n",
      "204/204 - 60s - loss: 0.1842 - auc: 0.8264 - binary_accuracy: 0.9766 - val_loss: 0.1626 - val_auc: 0.8751 - val_binary_accuracy: 0.9818 - lr: 1.6000e-04\n",
      "Epoch 11/15\n",
      "204/204 - 59s - loss: 0.1824 - auc: 0.8405 - binary_accuracy: 0.9754 - val_loss: 0.1628 - val_auc: 0.8722 - val_binary_accuracy: 0.9823 - lr: 1.6000e-04\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "204/204 - 59s - loss: 0.1750 - auc: 0.8435 - binary_accuracy: 0.9763 - val_loss: 0.1655 - val_auc: 0.8669 - val_binary_accuracy: 0.9800 - lr: 1.6000e-04\n",
      "Epoch 13/15\n",
      "204/204 - 61s - loss: 0.1781 - auc: 0.8490 - binary_accuracy: 0.9760 - val_loss: 0.1664 - val_auc: 0.8800 - val_binary_accuracy: 0.9816 - lr: 6.4000e-05\n",
      "Epoch 14/15\n",
      "204/204 - 61s - loss: 0.1745 - auc: 0.8534 - binary_accuracy: 0.9754 - val_loss: 0.1667 - val_auc: 0.8845 - val_binary_accuracy: 0.9816 - lr: 6.4000e-05\n",
      "Epoch 15/15\n",
      "204/204 - 59s - loss: 0.1722 - auc: 0.8553 - binary_accuracy: 0.9754 - val_loss: 0.1938 - val_auc: 0.8710 - val_binary_accuracy: 0.9764 - lr: 6.4000e-05\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\n",
      "Epoch 1/15\n",
      "204/204 - 81s - loss: 0.5316 - auc: 0.5716 - binary_accuracy: 0.8293 - val_loss: 0.2308 - val_auc: 0.6837 - val_binary_accuracy: 0.9820 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "204/204 - 56s - loss: 0.2830 - auc: 0.6409 - binary_accuracy: 0.9551 - val_loss: 0.7899 - val_auc: 0.6075 - val_binary_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "204/204 - 58s - loss: 0.2145 - auc: 0.7534 - binary_accuracy: 0.9698 - val_loss: 0.2022 - val_auc: 0.8129 - val_binary_accuracy: 0.9732 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "204/204 - 57s - loss: 0.2231 - auc: 0.7526 - binary_accuracy: 0.9708 - val_loss: 0.2680 - val_auc: 0.8025 - val_binary_accuracy: 0.9788 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "204/204 - 59s - loss: 0.2051 - auc: 0.7919 - binary_accuracy: 0.9734 - val_loss: 0.1855 - val_auc: 0.8203 - val_binary_accuracy: 0.9808 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "204/204 - 57s - loss: 0.2115 - auc: 0.7884 - binary_accuracy: 0.9759 - val_loss: 0.2074 - val_auc: 0.7533 - val_binary_accuracy: 0.9822 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "204/204 - 59s - loss: 0.2221 - auc: 0.7700 - binary_accuracy: 0.9725 - val_loss: 0.4062 - val_auc: 0.7206 - val_binary_accuracy: 0.9824 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "204/204 - 60s - loss: 0.1954 - auc: 0.8089 - binary_accuracy: 0.9769 - val_loss: 0.1812 - val_auc: 0.8584 - val_binary_accuracy: 0.9816 - lr: 4.0000e-04\n",
      "Epoch 9/15\n",
      "204/204 - 59s - loss: 0.1895 - auc: 0.8237 - binary_accuracy: 0.9758 - val_loss: 0.1740 - val_auc: 0.8625 - val_binary_accuracy: 0.9685 - lr: 4.0000e-04\n",
      "Epoch 10/15\n",
      "204/204 - 59s - loss: 0.1876 - auc: 0.8312 - binary_accuracy: 0.9770 - val_loss: 0.1681 - val_auc: 0.8645 - val_binary_accuracy: 0.9796 - lr: 4.0000e-04\n",
      "Epoch 11/15\n",
      "204/204 - 58s - loss: 0.1914 - auc: 0.8337 - binary_accuracy: 0.9758 - val_loss: 0.1730 - val_auc: 0.8589 - val_binary_accuracy: 0.9819 - lr: 4.0000e-04\n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "204/204 - 58s - loss: 0.1881 - auc: 0.8254 - binary_accuracy: 0.9771 - val_loss: 0.1710 - val_auc: 0.8578 - val_binary_accuracy: 0.9813 - lr: 4.0000e-04\n",
      "Epoch 13/15\n",
      "204/204 - 60s - loss: 0.1817 - auc: 0.8481 - binary_accuracy: 0.9753 - val_loss: 0.1647 - val_auc: 0.8748 - val_binary_accuracy: 0.9794 - lr: 1.6000e-04\n",
      "Epoch 14/15\n",
      "204/204 - 60s - loss: 0.1843 - auc: 0.8321 - binary_accuracy: 0.9755 - val_loss: 0.1708 - val_auc: 0.8777 - val_binary_accuracy: 0.9811 - lr: 1.6000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "204/204 - 58s - loss: 0.1865 - auc: 0.8311 - binary_accuracy: 0.9739 - val_loss: 0.1726 - val_auc: 0.8671 - val_binary_accuracy: 0.9807 - lr: 1.6000e-04\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\n",
      "Epoch 1/15\n",
      "204/204 - 79s - loss: 0.5070 - auc: 0.6244 - binary_accuracy: 0.8304 - val_loss: 0.3025 - val_auc: 0.7230 - val_binary_accuracy: 0.9127 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "204/204 - 56s - loss: 0.2935 - auc: 0.6382 - binary_accuracy: 0.9613 - val_loss: 0.2711 - val_auc: 0.5064 - val_binary_accuracy: 0.9816 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "204/204 - 56s - loss: 0.2425 - auc: 0.7099 - binary_accuracy: 0.9702 - val_loss: 0.2936 - val_auc: 0.6127 - val_binary_accuracy: 0.9823 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "204/204 - 59s - loss: 0.2226 - auc: 0.7354 - binary_accuracy: 0.9750 - val_loss: 0.1950 - val_auc: 0.8245 - val_binary_accuracy: 0.9823 - lr: 4.0000e-04\n",
      "Epoch 5/15\n",
      "204/204 - 58s - loss: 0.2042 - auc: 0.7853 - binary_accuracy: 0.9763 - val_loss: 0.1888 - val_auc: 0.8138 - val_binary_accuracy: 0.9823 - lr: 4.0000e-04\n",
      "Epoch 6/15\n",
      "204/204 - 59s - loss: 0.2039 - auc: 0.7818 - binary_accuracy: 0.9771 - val_loss: 0.1794 - val_auc: 0.8468 - val_binary_accuracy: 0.9772 - lr: 4.0000e-04\n",
      "Epoch 7/15\n",
      "204/204 - 57s - loss: 0.2112 - auc: 0.7872 - binary_accuracy: 0.9754 - val_loss: 0.1837 - val_auc: 0.8291 - val_binary_accuracy: 0.9821 - lr: 4.0000e-04\n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "204/204 - 58s - loss: 0.2016 - auc: 0.8083 - binary_accuracy: 0.9762 - val_loss: 0.2106 - val_auc: 0.8269 - val_binary_accuracy: 0.9816 - lr: 4.0000e-04\n",
      "Epoch 9/15\n",
      "204/204 - 58s - loss: 0.1834 - auc: 0.8294 - binary_accuracy: 0.9758 - val_loss: 0.1906 - val_auc: 0.8345 - val_binary_accuracy: 0.9797 - lr: 1.6000e-04\n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.40000042039901e-05.\n",
      "204/204 - 59s - loss: 0.1869 - auc: 0.8296 - binary_accuracy: 0.9747 - val_loss: 0.2336 - val_auc: 0.8428 - val_binary_accuracy: 0.9810 - lr: 1.6000e-04\n",
      "Epoch 11/15\n",
      "204/204 - 60s - loss: 0.1846 - auc: 0.8332 - binary_accuracy: 0.9771 - val_loss: 0.1735 - val_auc: 0.8577 - val_binary_accuracy: 0.9792 - lr: 6.4000e-05\n",
      "Epoch 12/15\n",
      "204/204 - 60s - loss: 0.1816 - auc: 0.8368 - binary_accuracy: 0.9773 - val_loss: 0.1711 - val_auc: 0.8587 - val_binary_accuracy: 0.9757 - lr: 6.4000e-05\n",
      "Epoch 13/15\n",
      "204/204 - 61s - loss: 0.1823 - auc: 0.8483 - binary_accuracy: 0.9752 - val_loss: 0.1721 - val_auc: 0.8597 - val_binary_accuracy: 0.9800 - lr: 6.4000e-05\n",
      "Epoch 14/15\n",
      "204/204 - 58s - loss: 0.1781 - auc: 0.8526 - binary_accuracy: 0.9744 - val_loss: 0.1720 - val_auc: 0.8581 - val_binary_accuracy: 0.9803 - lr: 6.4000e-05\n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.560000284574926e-05.\n",
      "204/204 - 59s - loss: 0.1784 - auc: 0.8458 - binary_accuracy: 0.9757 - val_loss: 0.1741 - val_auc: 0.8565 - val_binary_accuracy: 0.9801 - lr: 6.4000e-05\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\n",
      "Epoch 1/15\n",
      "204/204 - 81s - loss: 0.5347 - auc: 0.5900 - binary_accuracy: 0.8217 - val_loss: 0.5857 - val_auc: 0.5355 - val_binary_accuracy: 0.9333 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "204/204 - 57s - loss: 0.2699 - auc: 0.6626 - binary_accuracy: 0.9575 - val_loss: 0.8298 - val_auc: 0.6911 - val_binary_accuracy: 0.8317 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "204/204 - 58s - loss: 0.2484 - auc: 0.6880 - binary_accuracy: 0.9665 - val_loss: 0.1953 - val_auc: 0.8063 - val_binary_accuracy: 0.9797 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "204/204 - 56s - loss: 0.2163 - auc: 0.7470 - binary_accuracy: 0.9742 - val_loss: 0.2223 - val_auc: 0.7737 - val_binary_accuracy: 0.9820 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "204/204 - 59s - loss: 0.2039 - auc: 0.7799 - binary_accuracy: 0.9749 - val_loss: 0.2539 - val_auc: 0.8394 - val_binary_accuracy: 0.9823 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "204/204 - 59s - loss: 0.1933 - auc: 0.8163 - binary_accuracy: 0.9755 - val_loss: 0.1803 - val_auc: 0.8571 - val_binary_accuracy: 0.9820 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "204/204 - 60s - loss: 0.1883 - auc: 0.8231 - binary_accuracy: 0.9763 - val_loss: 0.1641 - val_auc: 0.8749 - val_binary_accuracy: 0.9811 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "204/204 - 57s - loss: 0.2404 - auc: 0.7269 - binary_accuracy: 0.9723 - val_loss: 0.3602 - val_auc: 0.6260 - val_binary_accuracy: 0.8834 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "204/204 - 58s - loss: 0.2261 - auc: 0.6962 - binary_accuracy: 0.9794 - val_loss: 0.1910 - val_auc: 0.7929 - val_binary_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "204/204 - 58s - loss: 0.2213 - auc: 0.7314 - binary_accuracy: 0.9791 - val_loss: 0.1890 - val_auc: 0.8123 - val_binary_accuracy: 0.9820 - lr: 4.0000e-04\n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00016000000759959222.\n",
      "204/204 - 58s - loss: 0.2018 - auc: 0.7573 - binary_accuracy: 0.9816 - val_loss: 0.1902 - val_auc: 0.8070 - val_binary_accuracy: 0.9785 - lr: 4.0000e-04\n",
      "Epoch 12/15\n",
      "Restoring model weights from the end of the best epoch.\n",
      "204/204 - 62s - loss: 0.2020 - auc: 0.7736 - binary_accuracy: 0.9802 - val_loss: 0.1866 - val_auc: 0.8209 - val_binary_accuracy: 0.9820 - lr: 1.6000e-04\n",
      "Epoch 00012: early stopping\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Computing predictions...\n",
      "Generating submission.csv file...\n",
      "Submission file generated\n"
     ]
    }
   ],
   "source": [
    "def train_and_predict(SUB, folds = 5):\n",
    "    \n",
    "    models = []\n",
    "    oof_image_name = []\n",
    "    oof_target = []\n",
    "    oof_prediction = []\n",
    "    \n",
    "    # seed everything\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    kfold = KFold(folds, shuffle = True, random_state = SEED)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        train_dataset = get_training_dataset([TRAINING_FILENAMES[x] for x in trn_ind], labeled = True, ordered = False)\n",
    "        val_dataset = get_validation_dataset([TRAINING_FILENAMES[x] for x in val_ind], labeled = True, ordered = True)\n",
    "        K.clear_session()\n",
    "        model = get_model()\n",
    "        # using early stopping using val loss\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 5, \n",
    "                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n",
    "        # lr scheduler\n",
    "        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_auc', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'max')\n",
    "        history = model.fit(train_dataset, \n",
    "                            steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                            epochs = EPOCHS,\n",
    "                            callbacks = [early_stopping, cb_lr_schedule],\n",
    "                            validation_data = val_dataset,\n",
    "                            verbose = 2)\n",
    "        models.append(model)\n",
    "        \n",
    "        # want to predict the validation set and save them for stacking\n",
    "        number_of_files = count_data_items([TRAINING_FILENAMES[x] for x in val_ind])\n",
    "        dataset = get_data_full([TRAINING_FILENAMES[x] for x in val_ind])\n",
    "        # get the image name\n",
    "        image_name = dataset.map(lambda image, image_name, target: image_name).unbatch()\n",
    "        image_name = next(iter(image_name.batch(number_of_files))).numpy().astype('U')\n",
    "        # get the real target\n",
    "        target = dataset.map(lambda image, image_name, target: target).unbatch()\n",
    "        target = next(iter(target.batch(number_of_files))).numpy()\n",
    "        # predict the validation set\n",
    "        image = dataset.map(lambda image, image_name, target: image)\n",
    "        probabilities = model.predict(image)\n",
    "        oof_image_name.extend(list(image_name))\n",
    "        oof_target.extend(list(target))\n",
    "        oof_prediction.extend(list(np.concatenate(probabilities)))\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    # save oof predictions\n",
    "    oof_df = pd.DataFrame({'image_name': oof_image_name, 'target': oof_target, 'predictions': oof_prediction})\n",
    "    oof_df.to_csv('EfficientNetB3_384.csv', index = False)\n",
    "        \n",
    "    # since we are splitting the dataset and iterating separately on images and ids, order matters.\n",
    "    test_ds = get_test_dataset(TEST_FILENAMES, labeled = False, ordered = True)\n",
    "    test_images_ds = test_ds.map(lambda image, image_name: image)\n",
    "    \n",
    "    print('Computing predictions...')\n",
    "    probabilities = np.average([np.concatenate(models[i].predict(test_images_ds)) for i in range(folds)], axis = 0)\n",
    "    print('Generating submission.csv file...')\n",
    "    test_ids_ds = test_ds.map(lambda image, image_name: image_name).unbatch()\n",
    "    # all in one batch\n",
    "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "    pred_df = pd.DataFrame({'image_name': test_ids, 'target': probabilities})\n",
    "    SUB.drop('target', inplace = True, axis = 1)\n",
    "    SUB = SUB.merge(pred_df, on = 'image_name')\n",
    "    SUB.to_csv('submission.csv', index = False)\n",
    "    print('Submission file generated')\n",
    "    \n",
    "    return oof_target, oof_prediction\n",
    "    \n",
    "oof_target, oof_prediction = train_and_predict(submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submission\n",
    "The below code creates submission csv file that will be uploaded on kaggle. **This model generated public score=0.8856**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0052060</td>\n",
       "      <td>0.164781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0052349</td>\n",
       "      <td>0.114981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0058510</td>\n",
       "      <td>0.101569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0073313</td>\n",
       "      <td>0.079869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0073502</td>\n",
       "      <td>0.258102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name    target\n",
       "0  ISIC_0052060  0.164781\n",
       "1  ISIC_0052349  0.114981\n",
       "2  ISIC_0058510  0.101569\n",
       "3  ISIC_0073313  0.079869\n",
       "4  ISIC_0073502  0.258102"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('submission_tpu_effnet.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Tensor_processing_unit\n",
    "2. Cutmix paper https://arxiv.org/abs/1905.04899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

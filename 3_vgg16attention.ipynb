{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 with Attention \n",
    "\n",
    " <u>**Contents**</u>-\n",
    "1. Why Attention?\n",
    "2. Load Libraries\n",
    "3. Prepare the Data\n",
    "4. Build the model Architecture\n",
    "5. Visualize Attentions\n",
    "6. Save Weights\n",
    "\n",
    "\n",
    "## 1. Why Attention?\n",
    "\n",
    "> * When training an image model, we want the model to be able to focus on important parts of the image. One way of accomplishing this is through **trainable attention** mechanisms. \n",
    "> * In our case we are dealing with lesion images and it becomes all the more necessary to be able to **interpret** the model.It is important to understand which part of the image contributes more towards the cancer  being classified benign/malignant.\n",
    "> * Post-hoc analysis like **Grad-CAM** are not the same as attention. They are not intended to change the way the model learns, or to change what the model learns. They are applied to an already-trained model with fixed weights, and are intended solely to provide insight into the modelâ€™s decisions.\n",
    "\n",
    "## 2. Load Libraries\n",
    "\n",
    "The below code imports the necessary libraries used. It also seeds basic parameters for reproductibility of results and also point the Train & Test locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.utils as utils\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.metrics import auc,roc_auc_score\n",
    "device = torch.device(\"cpu\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    \n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    " \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "train_dir='../input/melanoma-merged-external-data-512x512-jpeg/512x512-dataset-melanoma/512x512-dataset-melanoma/'\n",
    "test_dir='/kaggle/input/siim-isic-melanoma-classification/jpeg/test/'\n",
    "train=pd.read_csv('../input/melanoma-merged-external-data-512x512-jpeg/marking.csv')\n",
    "test=pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\n",
    "submission=pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the Data\n",
    "We'll prepare the data as usual in the below code.\n",
    "\n",
    "Certain preprocessing that we do for both Train & Test data are:\n",
    "* Train on only a sample of data.\n",
    "* Rotate the image by 10 degrees, reverse 50% of Train images, Resize,Normalize,CenterCrop train and test images.\n",
    "* Data Augmentation(Random Rotaion/ Horizontal Flip) on train images.\n",
    "* We also split the Train into ratio 80%:20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the data a bit\n",
    "df_0=train[train['target']==0].sample(6000,random_state=42)\n",
    "df_1=train[train['target']==1]\n",
    "train=pd.concat([df_0,df_1])\n",
    "train=train.reset_index()\n",
    "\n",
    "\n",
    "#prepare train data\n",
    "labels=[]\n",
    "data=[]\n",
    "for i in range(train.shape[0]):\n",
    "    data.append(train_dir + train['image_id'].iloc[i]+'.jpg')\n",
    "    labels.append(train['target'].iloc[i])\n",
    "df=pd.DataFrame(data)\n",
    "df.columns=['images']\n",
    "df['target']=labels\n",
    "\n",
    "\n",
    "#Prepare test data\n",
    "\n",
    "test_data=[]\n",
    "for i in range(test.shape[0]):\n",
    "    test_data.append(test_dir + test['image_name'].iloc[i]+'.jpg')\n",
    "df_test=pd.DataFrame(test_data)\n",
    "df_test.columns=['images']\n",
    "\n",
    "# Split train into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['images'],df['target'], test_size=0.2, random_state=1234)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code has 3 Dataloaders each for Train, Val & Test. These load the data in batch sizes of 100, 50, 50 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data_paths,labels,transform=None,mode='train'):\n",
    "         self.data=data_paths\n",
    "         self.labels=labels\n",
    "         self.transform=transform\n",
    "         self.mode=mode\n",
    "    def __len__(self):\n",
    "       return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.data[idx]\n",
    "        img = cv2.imread(img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img=Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "          img = self.transform(img)\n",
    "        img=img.cuda()\n",
    "        \n",
    "        if self.mode=='test':\n",
    "            return img,img_name\n",
    "        else:\n",
    "           \n",
    "            labels = torch.tensor(self.labels[idx]).cuda()\n",
    "\n",
    "            return img, labels\n",
    "\n",
    "train_dataset=ImageDataset(data_paths=X_train.values,labels=y_train.values,transform=train_transform)\n",
    "val_dataset=ImageDataset(data_paths=X_val.values,labels=y_val.values,transform=test_transform)\n",
    "test_dataset=ImageDataset(data_paths=df_test['images'].values,labels=None,transform=test_transform,mode='test')\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=100,shuffle=True)\n",
    "val_loader=DataLoader(val_dataset,batch_size=50,shuffle=False)\n",
    "test_loader=DataLoader(test_dataset,batch_size=50,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the model Architecture : VGG16 with Attention \n",
    "\n",
    "![](https://i.imgur.com/MIT3Wd8.png)\n",
    "\n",
    "* VGG-16 is the backbone network without any dense layers.\n",
    "* Two attention modules are applied (the gray blocks). The output of intermediate feature maps(pool-3 and pool-4) are used to infer attention maps. Output of pool-5 serves as a form of global-guidance because the last stage feature contains the most abstracted and compressed information over the entire image.\n",
    "* The three feature vectors (green blocks) are computed via global average pooling and are concatenated together to form the final feature vector, which serves as the input to the classification layer(not shown here).\n",
    "\n",
    "Below is a class defining the attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.up_factor = up_factor\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
    "    def forward(self, l, g):\n",
    "        N, C, W, H = l.size()\n",
    "        l_ = self.W_l(l)\n",
    "        g_ = self.W_g(g)\n",
    "        if self.up_factor > 1:\n",
    "            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)\n",
    "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
    "        \n",
    "        # compute attn map\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        # re-weight the local feature\n",
    "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
    "        if self.normalize_attn:\n",
    "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
    "        else:\n",
    "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n",
    "        return a, output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What goes on inside an Attention Layer can be explained by this figure.\n",
    "\n",
    "![](https://i.imgur.com/npJMDjq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The intermediate feature vector(F) is the output of pool-3 or pool-4 and the global feature vector (output of pool-5) are fed as input to the attention layer.\n",
    "* Both the feature vectors pass through a convolution layer. When the spatial size of global and intermediate features are different, feature upsampling is done via bilinear interpolation. The *up_factor* determines by what factor is the convoluted global feature vector has to be upscaled.\n",
    "* After that an element wise sum is done followed by a convolution operation that just reduces the 256 channels to 1.\n",
    "* This is then fed into a Softmax layer, which gives us a normalized Attention map (A).Each scalar element in A represents the degree of attention to the corresponding spatial feature vector in F.\n",
    "* The new feature vector $\\hat{F}$ is then computed by *pixel-wise* multiplication. That is, each feature vector $f_{i}$ is multiplied by the attention element $a_{i}$.\n",
    "* So, the attention map A and the new feature vector $\\hat{F}$ are the outputs of the Attention Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnVGG(nn.Module):\n",
    "    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n",
    "        super(AttnVGG, self).__init__()\n",
    "        net = models.vgg16_bn(pretrained=True)\n",
    "        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n",
    "        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n",
    "        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n",
    "        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n",
    "        self.pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dpt = None\n",
    "        if dropout is not None:\n",
    "            self.dpt = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)\n",
    "        \n",
    "       # initialize the attention blocks defined above\n",
    "        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)\n",
    "        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)\n",
    "        \n",
    "       \n",
    "        self.reset_parameters(self.cls)\n",
    "        self.reset_parameters(self.attn1)\n",
    "        self.reset_parameters(self.attn2)\n",
    "    def reset_parameters(self, module):\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1.)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0., 0.01)\n",
    "                nn.init.constant_(m.bias, 0.)\n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1(x)       # /1\n",
    "        pool1 = F.max_pool2d(block1, 2, 2) # /2\n",
    "        block2 = self.conv_block2(pool1)   # /2\n",
    "        pool2 = F.max_pool2d(block2, 2, 2) # /4\n",
    "        block3 = self.conv_block3(pool2)   # /4\n",
    "        pool3 = F.max_pool2d(block3, 2, 2) # /8\n",
    "        block4 = self.conv_block4(pool3)   # /8\n",
    "        pool4 = F.max_pool2d(block4, 2, 2) # /16\n",
    "        block5 = self.conv_block5(pool4)   # /16\n",
    "        pool5 = F.max_pool2d(block5, 2, 2) # /32\n",
    "        N, __, __, __ = pool5.size()\n",
    "        \n",
    "        g = self.pool(pool5).view(N,512)\n",
    "        a1, g1 = self.attn1(pool3, pool5)\n",
    "        a2, g2 = self.attn2(pool4, pool5)\n",
    "        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n",
    "        if self.dpt is not None:\n",
    "            g_hat = self.dpt(g_hat)\n",
    "        out = self.cls(g_hat)\n",
    "\n",
    "        return [out, a1, a2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The architecture of VGG16 is kept mostly the same except the Dense layers are removed.\n",
    "* We pass pool-3 and pool-4 through the attention layer to get $\\hat{F}_{3}$ and $\\hat{F}_{4}$ .\n",
    "* $\\hat{F}_{3}$ , $\\hat{F}_{4}$  and G(pool-5) are concatenated and fed into the final classification layer.\n",
    "* The whole network is trained end-to-end.\n",
    "\n",
    "The below code builds the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnVGG(num_classes=1, normalize_attn=True)\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use focal loss rather than regular Binary Cross-Entropy loss as our data is Imbalanced and focal loss can automatically down-weight easy samples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code trains 2 epochs of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 2\n",
    "results= [] \n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_auc=[]\n",
    "val_auc=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    train_preds=[]\n",
    "    train_targets=[]\n",
    "    auc_train=[]\n",
    "    loss_epoch_train=[]\n",
    "    loss_epoch_test=[]\n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "        \n",
    "        b+=1\n",
    "        y_pred,_,_=model(X_train)\n",
    "        loss = criterion(torch.sigmoid(y_pred.type(torch.FloatTensor)), y_train.type(torch.FloatTensor))   \n",
    "        loss_epoch_train.append(loss.item())\n",
    "        # For plotting purpose\n",
    "        if (i==1):\n",
    "            if (b==19):\n",
    "                I_train = utils.make_grid(X_train[0:8,:,:,:], nrow=8, normalize=True, scale_each=True)\n",
    "                __, a1, a2 = model(X_train[0:8,:,:,:])\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(epochs==2):\n",
    "            results.append(y_pred)\n",
    "                 \n",
    "    try:\n",
    "        auc_train=roc_auc_score(y_train.detach().to(device).numpy(),torch.sigmoid(y_pred).detach().to(device).numpy())\n",
    "    except:\n",
    "        auc_train=0\n",
    "    train_losses.append(np.mean(loss_epoch_train))\n",
    "    train_auc.append(auc_train)\n",
    "    print(f'epoch: {i:2}   loss: {np.mean(loss_epoch_train):10.8f} AUC  : {auc_train:10.8f} ')\n",
    "    # Run the testing batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            \n",
    "            y_val,_,_ = model(X_test)\n",
    "            loss = criterion(torch.sigmoid(y_val.type(torch.FloatTensor)), y_test.type(torch.FloatTensor))\n",
    "            loss_epoch_test.append(loss.item())\n",
    "    try:\n",
    "                                           \n",
    "        auc_val=roc_auc_score(y_test.detach().to(device).numpy(),torch.sigmoid(y_val).detach().to(device).numpy())\n",
    "    except:\n",
    "        auc_val=0\n",
    "    test_losses.append(np.mean(loss_epoch_test))\n",
    "    val_auc.append(auc_val)\n",
    "    print(f'Epoch: {i} Val Loss: {np.mean(loss_epoch_test):10.8f} AUC: {auc_val:10.8f} ')\n",
    "    if(epochs==2):\n",
    "            results.append(y_pred)\n",
    "    \n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Attention\n",
    "Now let's visualize the attention maps created by pool-3 and pool-4 to understand which part of the image are responsible for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(I_train,a,up_factor,no_attention=False):\n",
    "    img = I_train.permute((1,2,0)).cpu().numpy()\n",
    "    # compute the heatmap\n",
    "    if up_factor > 1:\n",
    "        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
    "    attn = utils.make_grid(a, nrow=8, normalize=True, scale_each=True)\n",
    "    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()\n",
    "    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)\n",
    "    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)\n",
    "    attn = np.float32(attn) / 255\n",
    "    # add the heatmap to the image\n",
    "    img=cv2.resize(img,(466,60))\n",
    "    if no_attention:\n",
    "        return torch.from_numpy(img)\n",
    "    else:\n",
    "        vis = 0.6 * img + 0.4 * attn\n",
    "        return torch.from_numpy(vis)\n",
    "    \n",
    "orig=visualize_attention(I_train,a1,up_factor=2,no_attention=True)\n",
    "first=visualize_attention(I_train,a1,up_factor=2,no_attention=False)\n",
    "second=visualize_attention(I_train,a2,up_factor=4,no_attention=False)\n",
    "\n",
    "fig, (ax1, ax2,ax3) = plt.subplots(3, 1,figsize=(10, 10))\n",
    "ax1.imshow(orig)\n",
    "ax2.imshow(first)\n",
    "ax3.imshow(second)\n",
    "ax1.title.set_text('Input Images')\n",
    "ax2.title.set_text('pool-3 attention')\n",
    "ax3.title.set_text('pool-4 attention')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/attn.png\" width=\"620\" height=\"420\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **The way this works for malignant images** :- The shallower layer (pool-3) tends to focus on more general and diffused areas, while the deeper layer (pool-4) is more concentrated, focusing on the lesion and avoiding irrelevant objects.\n",
    "> * But since most images in our case are benign, pool-3 tries to learn some areas but pool-4 eventually minimizes the activated regions because the image is benign.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving Weights\n",
    "Now let's make store the weights of the final layer so that it is used in AutoML later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {'image_name': [], 'target': []}\n",
    "for images, image_names in tqdm(train_dataset, total=len(train_dataset)):\n",
    "    preds=[]\n",
    "    with torch.no_grad():\n",
    "        outputs,_,_ = model(images)\n",
    "        a=torch.sigmoid(outputs).cpu().numpy()\n",
    "        for i in a:\n",
    "            preds.append(i[0])\n",
    "\n",
    "    result['image_name'].extend(image_names)\n",
    "    result['target'].extend(preds)\n",
    "\n",
    "submission = pd.DataFrame(result)\n",
    "submission['image_name']=[x[58:70] for x in train['image_name']]\n",
    "submission.to_csv('submission_train.csv',index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Notes\n",
    "\n",
    "* This kernel is a basic demonstration of how to use Attention mechanism with pretrained image models.\n",
    "* The paper also claims that due to the elimination of Dense Layers, number of parameters are greatly reduced and the network is lighter to train.\n",
    "* Tuning the hyperparameters, changing the backbone architecture,increasing training data might yield better results in this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* [https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf](https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf)\n",
    "* https://towardsdatascience.com/learn-to-pay-attention-trainable-visual-attention-in-cnns-87e2869f89f1\n",
    "* https://github.com/SaoYan/IPMI2019-AttnMel/tree/99e4a9b71717fb51f24d7994948b6a0e76bb8d58\n",
    "* https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
